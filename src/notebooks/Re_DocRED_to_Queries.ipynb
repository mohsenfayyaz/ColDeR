{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "gdown.download_folder(\"https://drive.google.com/drive/folders/1c5-0YwnoJx8NS6CV2f-NoTHR__BdkNqw\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/mohsenfayyaz/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# MOHSEN\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "login(os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Corpus: \n",
    "ReDocRED (test, val, train) + DocRED (train_distant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train_distant    101873\n",
      "train              3053\n",
      "test                500\n",
      "validation          500\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>vertexSet</th>\n",
       "      <th>labels</th>\n",
       "      <th>sents</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>[[{'name': 'Loud', 'pos': [23, 24], 'sent_id': 1, 'type': 'MISC', 'global_pos': [41, 41], 'index...</td>\n",
       "      <td>[{'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]}, {'r': 'P175', 'h': 0, 't': 2, 'evidence': [0, 1...</td>\n",
       "      <td>[[The, Loud, Tour, was, the, fourth, overall, and, third, world, concert, tour, by, Barbadian, r...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vladimir Mitrofanovich Orlov</td>\n",
       "      <td>[[{'name': 'Vladimir Mitrofanovich Orlov', 'pos': [0, 3], 'sent_id': 0, 'type': 'PER', 'global_p...</td>\n",
       "      <td>[{'r': 'P69', 'h': 0, 't': 9, 'evidence': [1]}, {'r': 'P570', 'h': 0, 't': 25, 'evidence': [0, 7...</td>\n",
       "      <td>[[Vladimir, Mitrofanovich, Orlov, (, ), (, July, 15, ,, 1895, -, July, 28, ,, 1938, ), was, a, R...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ecuadorian Constituent Assembly</td>\n",
       "      <td>[[{'name': 'Ecuadorian Constituent Assembly', 'pos': [1, 4], 'sent_id': 0, 'type': 'ORG', 'globa...</td>\n",
       "      <td>[{'r': 'P17', 'h': 0, 't': 3, 'evidence': [0]}, {'r': 'P571', 'h': 0, 't': 1, 'evidence': [0]}, ...</td>\n",
       "      <td>[[The, Ecuadorian, Constituent, Assembly, was, a, 2007, –, 2008, constitutional, assembly, in, E...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bantustan</td>\n",
       "      <td>[[{'name': 'Bantustan', 'pos': [1, 2], 'sent_id': 0, 'type': 'LOC', 'global_pos': [1, 1], 'index...</td>\n",
       "      <td>[{'r': 'P1366', 'h': 3, 't': 4, 'evidence': [0]}, {'r': 'P1365', 'h': 4, 't': 3, 'evidence': [0]...</td>\n",
       "      <td>[[A, Bantustan, (, also, known, as, Bantu, homeland, ,, black, homeland, ,, black, state, or, si...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Long Hard Road Out of Hell</td>\n",
       "      <td>[[{'name': 'Long Hard Road Out of Hell', 'pos': [1, 7], 'sent_id': 0, 'type': 'MISC', 'global_po...</td>\n",
       "      <td>[{'r': 'P162', 'h': 0, 't': 7, 'evidence': [0, 1, 5]}, {'r': 'P162', 'h': 0, 't': 2, 'evidence':...</td>\n",
       "      <td>[[\", Long, Hard, Road, Out, of, Hell, \", is, a, song, by, American, rock, band, Marilyn, Manson,...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101868</th>\n",
       "      <td>Last Flag Flying</td>\n",
       "      <td>[[{'pos': [0, 3], 'type': 'MISC', 'sent_id': 0, 'name': 'Last Flag Flying'}, {'pos': [3, 6], 'ty...</td>\n",
       "      <td>[{'h': 0, 't': 1, 'r': 'P577', 'evidence': []}, {'h': 0, 't': 3, 'r': 'P57', 'evidence': []}, {'...</td>\n",
       "      <td>[[Last, Flag, Flying, is, a, 2017, American, comedy, -, drama, film, directed, by, Richard, Link...</td>\n",
       "      <td>train_distant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101869</th>\n",
       "      <td>Hillman Minx</td>\n",
       "      <td>[[{'pos': [1, 3], 'type': 'MISC', 'sent_id': 0, 'name': 'Hillman Minx'}], [{'pos': [11, 12], 'ty...</td>\n",
       "      <td>[{'h': 2, 't': 3, 'r': 'P576', 'evidence': []}, {'h': 2, 't': 25, 'r': 'P156', 'evidence': []}, ...</td>\n",
       "      <td>[[The, Hillman, Minx, was, a, mid, -, sized, family, car, that, British, car, maker, Hillman, pr...</td>\n",
       "      <td>train_distant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101870</th>\n",
       "      <td>Knute Nelson</td>\n",
       "      <td>[[{'pos': [0, 2], 'type': 'PER', 'sent_id': 0, 'name': 'Knute Nelson'}], [{'pos': [4, 6], 'type'...</td>\n",
       "      <td>[{'h': 0, 't': 2, 'r': 'P569', 'evidence': []}, {'h': 0, 't': 3, 'r': 'P570', 'evidence': []}, {...</td>\n",
       "      <td>[[Knute, Nelson, (, born, Knud, Evanger, ;, February, 2, ,, 1843, April, 28, ,, 1923, ), was, an...</td>\n",
       "      <td>train_distant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101871</th>\n",
       "      <td>Who Do You Think You Are? (book)</td>\n",
       "      <td>[[{'pos': [0, 6], 'type': 'MISC', 'sent_id': 0, 'name': 'Who Do You Think You Are'}], [{'pos': [...</td>\n",
       "      <td>[{'h': 1, 't': 3, 'r': 'P166', 'evidence': []}, {'h': 1, 't': 7, 'r': 'P27', 'evidence': []}, {'...</td>\n",
       "      <td>[[Who, Do, You, Think, You, Are, ?], [is, a, book, of, short, stories, by, Alice, Munro, ,, reci...</td>\n",
       "      <td>train_distant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101872</th>\n",
       "      <td>Neal Lane Bridge</td>\n",
       "      <td>[[{'pos': [0, 3], 'type': 'LOC', 'sent_id': 0, 'name': 'Neal Lane Bridge'}], [{'pos': [8, 10], '...</td>\n",
       "      <td>[{'h': 0, 't': 1, 'r': 'P131', 'evidence': []}, {'h': 0, 't': 2, 'r': 'P17', 'evidence': []}, {'...</td>\n",
       "      <td>[[Neal, Lane, Bridge, is, a, covered, bridge, in, Douglas, County, in, the, U.S., state, of, Ore...</td>\n",
       "      <td>train_distant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105926 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  \\\n",
       "0                              Loud Tour   \n",
       "1           Vladimir Mitrofanovich Orlov   \n",
       "2        Ecuadorian Constituent Assembly   \n",
       "3                              Bantustan   \n",
       "4             Long Hard Road Out of Hell   \n",
       "...                                  ...   \n",
       "101868                  Last Flag Flying   \n",
       "101869                      Hillman Minx   \n",
       "101870                      Knute Nelson   \n",
       "101871  Who Do You Think You Are? (book)   \n",
       "101872                  Neal Lane Bridge   \n",
       "\n",
       "                                                                                                  vertexSet  \\\n",
       "0       [[{'name': 'Loud', 'pos': [23, 24], 'sent_id': 1, 'type': 'MISC', 'global_pos': [41, 41], 'index...   \n",
       "1       [[{'name': 'Vladimir Mitrofanovich Orlov', 'pos': [0, 3], 'sent_id': 0, 'type': 'PER', 'global_p...   \n",
       "2       [[{'name': 'Ecuadorian Constituent Assembly', 'pos': [1, 4], 'sent_id': 0, 'type': 'ORG', 'globa...   \n",
       "3       [[{'name': 'Bantustan', 'pos': [1, 2], 'sent_id': 0, 'type': 'LOC', 'global_pos': [1, 1], 'index...   \n",
       "4       [[{'name': 'Long Hard Road Out of Hell', 'pos': [1, 7], 'sent_id': 0, 'type': 'MISC', 'global_po...   \n",
       "...                                                                                                     ...   \n",
       "101868  [[{'pos': [0, 3], 'type': 'MISC', 'sent_id': 0, 'name': 'Last Flag Flying'}, {'pos': [3, 6], 'ty...   \n",
       "101869  [[{'pos': [1, 3], 'type': 'MISC', 'sent_id': 0, 'name': 'Hillman Minx'}], [{'pos': [11, 12], 'ty...   \n",
       "101870  [[{'pos': [0, 2], 'type': 'PER', 'sent_id': 0, 'name': 'Knute Nelson'}], [{'pos': [4, 6], 'type'...   \n",
       "101871  [[{'pos': [0, 6], 'type': 'MISC', 'sent_id': 0, 'name': 'Who Do You Think You Are'}], [{'pos': [...   \n",
       "101872  [[{'pos': [0, 3], 'type': 'LOC', 'sent_id': 0, 'name': 'Neal Lane Bridge'}], [{'pos': [8, 10], '...   \n",
       "\n",
       "                                                                                                     labels  \\\n",
       "0       [{'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]}, {'r': 'P175', 'h': 0, 't': 2, 'evidence': [0, 1...   \n",
       "1       [{'r': 'P69', 'h': 0, 't': 9, 'evidence': [1]}, {'r': 'P570', 'h': 0, 't': 25, 'evidence': [0, 7...   \n",
       "2       [{'r': 'P17', 'h': 0, 't': 3, 'evidence': [0]}, {'r': 'P571', 'h': 0, 't': 1, 'evidence': [0]}, ...   \n",
       "3       [{'r': 'P1366', 'h': 3, 't': 4, 'evidence': [0]}, {'r': 'P1365', 'h': 4, 't': 3, 'evidence': [0]...   \n",
       "4       [{'r': 'P162', 'h': 0, 't': 7, 'evidence': [0, 1, 5]}, {'r': 'P162', 'h': 0, 't': 2, 'evidence':...   \n",
       "...                                                                                                     ...   \n",
       "101868  [{'h': 0, 't': 1, 'r': 'P577', 'evidence': []}, {'h': 0, 't': 3, 'r': 'P57', 'evidence': []}, {'...   \n",
       "101869  [{'h': 2, 't': 3, 'r': 'P576', 'evidence': []}, {'h': 2, 't': 25, 'r': 'P156', 'evidence': []}, ...   \n",
       "101870  [{'h': 0, 't': 2, 'r': 'P569', 'evidence': []}, {'h': 0, 't': 3, 'r': 'P570', 'evidence': []}, {...   \n",
       "101871  [{'h': 1, 't': 3, 'r': 'P166', 'evidence': []}, {'h': 1, 't': 7, 'r': 'P27', 'evidence': []}, {'...   \n",
       "101872  [{'h': 0, 't': 1, 'r': 'P131', 'evidence': []}, {'h': 0, 't': 2, 'r': 'P17', 'evidence': []}, {'...   \n",
       "\n",
       "                                                                                                      sents  \\\n",
       "0       [[The, Loud, Tour, was, the, fourth, overall, and, third, world, concert, tour, by, Barbadian, r...   \n",
       "1       [[Vladimir, Mitrofanovich, Orlov, (, ), (, July, 15, ,, 1895, -, July, 28, ,, 1938, ), was, a, R...   \n",
       "2       [[The, Ecuadorian, Constituent, Assembly, was, a, 2007, –, 2008, constitutional, assembly, in, E...   \n",
       "3       [[A, Bantustan, (, also, known, as, Bantu, homeland, ,, black, homeland, ,, black, state, or, si...   \n",
       "4       [[\", Long, Hard, Road, Out, of, Hell, \", is, a, song, by, American, rock, band, Marilyn, Manson,...   \n",
       "...                                                                                                     ...   \n",
       "101868  [[Last, Flag, Flying, is, a, 2017, American, comedy, -, drama, film, directed, by, Richard, Link...   \n",
       "101869  [[The, Hillman, Minx, was, a, mid, -, sized, family, car, that, British, car, maker, Hillman, pr...   \n",
       "101870  [[Knute, Nelson, (, born, Knud, Evanger, ;, February, 2, ,, 1843, April, 28, ,, 1923, ), was, an...   \n",
       "101871  [[Who, Do, You, Think, You, Are, ?], [is, a, book, of, short, stories, by, Alice, Munro, ,, reci...   \n",
       "101872  [[Neal, Lane, Bridge, is, a, covered, bridge, in, Douglas, County, in, the, U.S., state, of, Ore...   \n",
       "\n",
       "                split  \n",
       "0                test  \n",
       "1                test  \n",
       "2                test  \n",
       "3                test  \n",
       "4                test  \n",
       "...               ...  \n",
       "101868  train_distant  \n",
       "101869  train_distant  \n",
       "101870  train_distant  \n",
       "101871  train_distant  \n",
       "101872  train_distant  \n",
       "\n",
       "[105926 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def load_data():\n",
    "    splits = {\n",
    "        'test': 'test_revised.json',\n",
    "        'validation': 'dev_revised.json',\n",
    "        'train': 'train_revised.json',\n",
    "    }\n",
    "    raw_df = pd.DataFrame()\n",
    "    for split in splits.keys():\n",
    "        new_df = pd.read_json(\"hf://datasets/tonytan48/Re-DocRED/\" + splits[split])\n",
    "        new_df[\"split\"] = split\n",
    "        raw_df = pd.concat([raw_df, new_df])\n",
    "\n",
    "    new_df = pd.read_json(\"./DocRED/train_distant.json\")\n",
    "    new_df[\"split\"] = \"train_distant\"\n",
    "    raw_df = pd.concat([raw_df, new_df])\n",
    "    return raw_df\n",
    "\n",
    "raw_df = load_data()\n",
    "print(raw_df[\"split\"].value_counts())\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5688037a00b43dda1f0e85222031d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hffs-9n6mkjo3:   0%|          | 0.00/93.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UPLOAD ON HF\n",
    "raw_df.to_pickle(f\"hf://datasets/Retriever-Contextualization/datasets/Re-DocRED/corpus_all.pkl\")\n",
    "raw_df.to_pickle(f\"hf://datasets/Retriever-Contextualization/datasets/Re-DocRED/corpus_all.pkl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7eb58e2e184c47ab2261cda3b2b566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Queries:   0%|          | 0/105926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71aa76f8b43c4fc2bd1f73447a8d7b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1552195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def generate_query_question(head_entity, tail_entity, relation_type):\n",
    "    relation_mapping = {\n",
    "        'P6': f'Who is the head of government of {head_entity}?',                 # Invertible\n",
    "        'P17': f'Which country is {head_entity} associated with?',\n",
    "        'P19': f'Where was {head_entity} born?',\n",
    "        'P20': f'Where did {head_entity} die?',\n",
    "        'P22': f'Who is the father of {head_entity}?',\n",
    "        'P25': f'Who is the mother of {head_entity}?',\n",
    "        'P26': f'Who is the spouse of {head_entity}?',                            # Invertible\n",
    "        'P27': f'Which country is {head_entity} a citizen of?',                   # Could have multiple answers\n",
    "        'P30': f'Which continent is {head_entity} part of?',\n",
    "        'P31': f'What is {head_entity} an instance of?',                          # Could have multiple answers\n",
    "        'P35': f'Who is the head of state of {head_entity}?',                     # Invertible\n",
    "        'P36': f'What is the capital of {head_entity}?',                          # Invertible\n",
    "        'P37': f'What is the official language of {head_entity}?',\n",
    "        'P39': f'What position did {head_entity} hold?',                          # Could have multiple answers\n",
    "        'P40': f'Who are the children of {head_entity}?',                         # Could have multiple answers\n",
    "        'P50': f'Who is the author of {head_entity}?',                            # Could have multiple answers (not likely in DocRED)\n",
    "        'P54': f'Which sports team is {head_entity} a member of?',                # Could have multiple answers (not likely in DocRED)\n",
    "        'P57': f'Who directed {head_entity}?',                                    # Could have multiple answers (not likely in DocRED)\n",
    "        'P58': f'Who wrote the screenplay for {head_entity}?',                    # Could have multiple answers (not likely in DocRED)\n",
    "        'P69': f'Where was {head_entity} educated?',                              # Could have multiple answers\n",
    "        'P86': f'Who composed the music for {head_entity}?',\n",
    "        'P102': f'Which political party is {head_entity} a member of?',           # Could have multiple answers (not likely in DocRED)\n",
    "        'P108': f'Who is the employer of {head_entity}?',\n",
    "        'P112': f'Who founded {head_entity}?',                                    # Could have multiple answers, Invertible\n",
    "        'P118': f'Which league is {head_entity} part of?',\n",
    "        'P123': f'Who is the publisher of {head_entity}?',\n",
    "        'P127': f'Who owns {head_entity}?',                                       # Could have multiple answers\n",
    "        'P131': f'Which administrative territorial entity is {head_entity} located in?', # Could have multiple answers\n",
    "        'P136': f'What genre does {head_entity} belong to?',                      # Could have multiple answers (not likely in DocRED)\n",
    "        'P137': f'Who operates {head_entity}?',\n",
    "        'P140': f'What is the religion of {head_entity}?',                        # Could have multiple answers (not likely in DocRED)\n",
    "        # 'P150': f'What administrative territorial entity is contained within {head_entity}?', # Extreme Many-to-many (DISCARD THIS TYPE)\n",
    "        'P155': f'What precedes {head_entity}?',\n",
    "        'P156': f'What follows {head_entity}?',\n",
    "        'P159': f'Where is the headquarters of {head_entity} located?',\n",
    "        'P161': f'Who is a cast member of {head_entity}?',\n",
    "        'P162': f'Who produced {head_entity}?',                                   # Could have multiple answers\n",
    "        'P166': f'What award did {head_entity} receive?',                         # Could have multiple answers (not likely in DocRED)\n",
    "        'P170': f'Who created {head_entity}?',                                    # Could have multiple answers (not likely in DocRED)\n",
    "        'P171': f'What is the parent taxon of {head_entity}?',                    # Could have multiple answers (not likely in DocRED)\n",
    "        'P172': f'What is the ethnic group of {head_entity}?',                    # Could have multiple answers (not likely in DocRED)\n",
    "        'P175': f'Who performed {head_entity}?',\n",
    "        'P176': f'Who manufactured {head_entity}?',\n",
    "        'P178': f'Who developed {head_entity}?',\n",
    "        'P179': f'What series is {head_entity} part of?',\n",
    "        'P190': f'What is the sister city of {head_entity}?',\n",
    "        'P194': f'What is the legislative body of {head_entity}?',\n",
    "        'P205': f'What country is the basin of {head_entity}?',\n",
    "        'P206': f'Where is {head_entity} located in or next to a body of water?',\n",
    "        'P241': f'Which military branch is {head_entity} part of?',\n",
    "        'P264': f'Which record label is {head_entity} associated with?',\n",
    "        'P272': f'Which production company produced {head_entity}?',\n",
    "        'P276': f'Where is {head_entity} located?',\n",
    "        'P279': f'What is {head_entity} a subclass of?',\n",
    "        'P355': f'What is the subsidiary of {head_entity}?',                      # Could have multiple answers\n",
    "        'P361': f'What is {head_entity} a part of?',\n",
    "        'P364': f'What is the original language of {head_entity}?',\n",
    "        'P400': f'What platform is {head_entity} available on?',                  # Could have multiple answers\n",
    "        'P403': f'What is the mouth of the watercourse of {head_entity}?',\n",
    "        'P449': f'What is the original network of {head_entity}?',\n",
    "        'P463': f'Which organization is {head_entity} a member of?',\n",
    "        'P488': f'Who is the chairperson of {head_entity}?',\n",
    "        'P495': f'What is the country of origin of {head_entity}?',\n",
    "        'P527': f'What are the components of {head_entity}?',                     # Could have multiple answers\n",
    "        'P551': f'Where is the residence of {head_entity}?',\n",
    "        'P569': f'When was {head_entity} born?',\n",
    "        'P570': f'When did {head_entity} die?',\n",
    "        'P571': f'When was {head_entity} founded?',\n",
    "        'P576': f'When was {head_entity} dissolved or demolished?',\n",
    "        'P577': f'When was {head_entity} published?',\n",
    "        'P580': f'When did {head_entity} start?',\n",
    "        'P582': f'When did {head_entity} end?',\n",
    "        'P585': f'When did {head_entity} occur?',\n",
    "        'P607': f'What conflict was {head_entity} part of?',                      # Could have multiple answers\n",
    "        'P674': f'Who are the characters in {head_entity}?',                      # Could have multiple answers\n",
    "        'P676': f'Who wrote the lyrics for {head_entity}?',                       # Could have multiple answers\n",
    "        'P706': f'Where is {head_entity} located on a terrain feature?',\n",
    "        # 'P710': f'Who participated in {head_entity}?',                            # Extreme Many-to-many (DISCARD THIS TYPE)\n",
    "        'P737': f'Who influenced {head_entity}?',\n",
    "        'P740': f'Where was {head_entity} formed?',\n",
    "        'P749': f'What is the parent organization of {head_entity}?',\n",
    "        'P800': f'What is a notable work of {head_entity}?',\n",
    "        'P807': f'What is {head_entity} separated from?',\n",
    "        'P840': f'Where does the narrative of {head_entity} take place?',\n",
    "        'P937': f'Where did {head_entity} work?',\n",
    "        'P1001': f'Which jurisdiction does {head_entity} apply to?',\n",
    "        'P1056': f'What does {head_entity} produce?',\n",
    "        'P1198': f'What is the unemployment rate of {head_entity}?',\n",
    "        'P1336': f'What territory is claimed by {head_entity}?',\n",
    "        'P1344': f'What was {head_entity} a participant of?',\n",
    "        'P1365': f'What does {head_entity} replace?',\n",
    "        'P1366': f'What replaced {head_entity}?',\n",
    "        'P1376': f'What is {head_entity} the capital of?',                        # BEWARE OF THE DUPLICATION FOR FILTERING (FACT P36 is the same as this one) FIXED\n",
    "        'P1412': f'What languages are spoken, written, or signed by {head_entity}?',    # Could have multiple answers\n",
    "        'P1441': f'In what work does {head_entity} appear?',\n",
    "        'P3373': f'Who is the sibling of {head_entity}?'                          # Could have multiple answers, Invertible (based on each sibling)\n",
    "    }\n",
    "    return relation_mapping[relation_type]\n",
    "\n",
    "DISCARDED_RELATIONS = ['P150', 'P710']\n",
    "\n",
    "def create_query_dataset(redocred_data):\n",
    "    queries = []\n",
    "    duplicate_head_relation_to_titles = {}\n",
    "    import json\n",
    "    with open(\"./DocRED/rel_info.json\") as fd:\n",
    "        rel_info = json.load(fd)\n",
    "\n",
    "    for row in tqdm(redocred_data.to_dict(orient='records'), total=len(redocred_data), desc=\"Generating Queries\"):\n",
    "        sents = row['sents']\n",
    "        # context = \" \".join([\" \".join(sent) for sent in sents])\n",
    "        vertex_set = row['vertexSet']\n",
    "\n",
    "        for label_idx, relation in enumerate(row['labels']):\n",
    "            head_idx = relation['h']\n",
    "            tail_idx = relation['t']\n",
    "            relation_type = relation['r']\n",
    "            if relation_type in DISCARDED_RELATIONS:\n",
    "                continue\n",
    "\n",
    "            head_entity = vertex_set[head_idx]\n",
    "            tail_entity = vertex_set[tail_idx]\n",
    "\n",
    "            ### Head and Tail Names\n",
    "            head_entity_names = {entity['name'] for entity in head_entity}\n",
    "            tail_entity_names = {entity['name'] for entity in tail_entity}\n",
    "            head_entity_longest_name = max(head_entity_names, key=len)\n",
    "            tail_entity_longest_name = max(tail_entity_names, key=len)\n",
    "\n",
    "            ### Generate Questions\n",
    "            query_question = generate_query_question(head_entity_longest_name, tail_entity_longest_name, relation_type)\n",
    "\n",
    "            ### Filter Evidence Sentences\n",
    "            evidence_sent_ids = relation['evidence']\n",
    "            evidence_sents = [sents[sent_id] for sent_id in evidence_sent_ids]\n",
    "            head_entity_in_evidence = [entity for entity in head_entity if entity['sent_id'] in evidence_sent_ids]\n",
    "            tail_entity_in_evidence = [entity for entity in tail_entity if entity['sent_id'] in evidence_sent_ids]\n",
    "\n",
    "            ### Duplicates\n",
    "            for head_name in head_entity_names:\n",
    "                if (head_name, relation_type) not in duplicate_head_relation_to_titles:\n",
    "                    duplicate_head_relation_to_titles[(head_name, relation_type)] = set([row['title']])\n",
    "                else:\n",
    "                    duplicate_head_relation_to_titles[(head_name, relation_type)].add(row['title'])\n",
    "\n",
    "            query = {\n",
    "                **row,\n",
    "                # 'title': row['title'],\n",
    "                # 'vertexSet': row['vertexSet'],\n",
    "                # 'labels': row['labels'],\n",
    "                # 'sents': row['sents'],\n",
    "\n",
    "                'label': relation,\n",
    "                'label_idx': label_idx,\n",
    "                # 'context': context,\n",
    "\n",
    "                'head_entity': head_entity,\n",
    "                'tail_entity': tail_entity,\n",
    "                'head_entity_names': set(head_entity_names),\n",
    "                'tail_entity_names': set(tail_entity_names),\n",
    "                'head_entity_longest_name': head_entity_longest_name,\n",
    "                'tail_entity_longest_name': tail_entity_longest_name,\n",
    "\n",
    "                'head_entity_types': {e['type'] for e in head_entity},\n",
    "                'tail_entity_types': {e['type'] for e in tail_entity},\n",
    "\n",
    "                'evidence_sent_ids': evidence_sent_ids,\n",
    "                'evidence_sents': evidence_sents,\n",
    "                'head_entity_in_evidence': head_entity_in_evidence,\n",
    "                'tail_entity_in_evidence': tail_entity_in_evidence,\n",
    "\n",
    "                'relation': relation_type,\n",
    "                'relation_name': rel_info[relation_type],\n",
    "                'query_question': query_question\n",
    "            }\n",
    "            queries.append(query)\n",
    "\n",
    "    ### Find Duplicates (head, relation) with different titles\n",
    "    def find_duplicate_titles(title, head_names, relation):\n",
    "        duplicate_titles_row = list()\n",
    "        for head_name in head_names:\n",
    "            if (head_name, relation) in duplicate_head_relation_to_titles:\n",
    "                duplicate_titles_row.extend(duplicate_head_relation_to_titles[(head_name, relation)])\n",
    "        duplicate_titles_row = set(duplicate_titles_row)\n",
    "        duplicate_titles_row.remove(title)\n",
    "        return duplicate_titles_row\n",
    "\n",
    "    for q in tqdm(queries):\n",
    "        duplicate_titles = find_duplicate_titles(q[\"title\"], q[\"head_entity_names\"], q[\"relation\"])\n",
    "        q[\"duplicate_titles_len\"] = len(duplicate_titles)\n",
    "        q[\"duplicate_titles\"] = set(list(duplicate_titles)[:10])\n",
    "\n",
    "    query_df = pd.DataFrame(queries)\n",
    "    return query_df\n",
    "\n",
    "query_df = create_query_dataset(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/mohsenfayyaz/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c518737d7d4ae6839097691da39dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hffs-4qdrvdd3:   0%|          | 0.00/718M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c940a254d74c9f928de5cd5f1a7de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hffs-vv97u3fs:   0%|          | 0.00/149M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb655badf26c4d25b38e93927f9c7029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hffs-cc0wno9h:   0%|          | 0.00/6.54M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f744e1c8cdc48b4a42455c396dc1b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hffs-wfo_63p_:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UPLOAD\n",
    "query_df.to_pickle(f\"hf://datasets/Retriever-Contextualization/datasets/Re-DocRED/queries_all.pkl\")\n",
    "query_df.to_pickle(f\"hf://datasets/Retriever-Contextualization/datasets/Re-DocRED/queries_all.pkl.gz\")\n",
    "query_df[query_df[\"split\"] == \"test\"].to_pickle(f\"hf://datasets/Retriever-Contextualization/datasets/Re-DocRED/queries_test.pkl\")\n",
    "query_df[query_df[\"split\"] == \"test\"].to_pickle(f\"hf://datasets/Retriever-Contextualization/datasets/Re-DocRED/queries_test.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beir-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
