{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/mohsenfayyaz/.cache/huggingface/token\n",
      "Login successful\n",
      "CUDA_VISIBLE_DEVICES: 1 HF_HOME: /local1/mohsenfayyaz/.hfcache/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "import huggingface_hub as hf\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Union, Tuple\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 512)\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "load_dotenv()\n",
    "hf.login(os.environ[\"HF_TOKEN\"])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ[\"CUDA_VISIBLE_DEVICES\"], \"HF_HOME:\", os.environ[\"HF_HOME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset + DecompX Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hf/results/re-docred_facebook--dragon-plus-query-encoder_7170.pkl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATASET = \"re-docred_facebook--contriever-msmarco_7170.pkl\"\n",
    "DATASET = \"re-docred_facebook--dragon-plus-query-encoder_7170.pkl\"\n",
    "# DATASET = \"re-docred_OpenMatch--cocodr-base-msmarco_7170.pkl.gz\"\n",
    "\n",
    "hf.hf_hub_download(repo_id=\"Retriever-Contextualization/datasets\", filename=f\"results/{DATASET}\", repo_type=\"dataset\", local_dir=\"hf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'facebook/dragon-plus-query-encoder', 'query_model': 'facebook/dragon-plus-query-encoder', 'context_model': 'facebook/dragon-plus-context-encoder', 'pooling': 'cls', 'dataset': 're-docred', 'corpus_size': 105925, 'eval': {'ndcg': {'NDCG@1': 0.47685, 'NDCG@3': 0.52523, 'NDCG@5': 0.53646, 'NDCG@10': 0.54955, 'NDCG@100': 0.58002, 'NDCG@1000': 0.59556}, 'map': {'MAP@1': 0.47685, 'MAP@3': 0.51341, 'MAP@5': 0.51959, 'MAP@10': 0.52496, 'MAP@100': 0.53058, 'MAP@1000': 0.53109}, 'recall': {'Recall@1': 0.47685, 'Recall@3': 0.55941, 'Recall@5': 0.58689, 'Recall@10': 0.62748, 'Recall@100': 0.77741, 'Recall@1000': 0.90349}, 'precision': {'P@1': 0.47685, 'P@3': 0.18647, 'P@5': 0.11738, 'P@10': 0.06275, 'P@100': 0.00777, 'P@1000': 0.0009}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>gold_docs</th>\n",
       "      <th>gold_docs_text</th>\n",
       "      <th>scores_stats</th>\n",
       "      <th>scores_gold</th>\n",
       "      <th>scores_1000</th>\n",
       "      <th>predicted_docs_text_10</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>vertexSet</th>\n",
       "      <th>labels</th>\n",
       "      <th>sents</th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>label_idx</th>\n",
       "      <th>head_entity</th>\n",
       "      <th>tail_entity</th>\n",
       "      <th>head_entity_names</th>\n",
       "      <th>tail_entity_names</th>\n",
       "      <th>head_entity_longest_name</th>\n",
       "      <th>tail_entity_longest_name</th>\n",
       "      <th>head_entity_types</th>\n",
       "      <th>tail_entity_types</th>\n",
       "      <th>evidence_sent_ids</th>\n",
       "      <th>evidence_sents</th>\n",
       "      <th>head_entity_in_evidence</th>\n",
       "      <th>tail_entity_in_evidence</th>\n",
       "      <th>relation</th>\n",
       "      <th>relation_name</th>\n",
       "      <th>query_question</th>\n",
       "      <th>duplicate_titles_len</th>\n",
       "      <th>duplicate_titles</th>\n",
       "      <th>hit_rank</th>\n",
       "      <th>gold_doc</th>\n",
       "      <th>gold_doc_title</th>\n",
       "      <th>gold_doc_text</th>\n",
       "      <th>gold_doc_score</th>\n",
       "      <th>pred_doc</th>\n",
       "      <th>pred_doc_title</th>\n",
       "      <th>pred_doc_text</th>\n",
       "      <th>pred_doc_score</th>\n",
       "      <th>gold_doc_len</th>\n",
       "      <th>pred_doc_len</th>\n",
       "      <th>query_decompx_tokens</th>\n",
       "      <th>query_decompx_tokenizer_word_ids</th>\n",
       "      <th>query_decompx_cls_or_mean_pooled</th>\n",
       "      <th>query_decompx_tokens_dot_scores</th>\n",
       "      <th>query_decompx_decompx_last_layer_pooled</th>\n",
       "      <th>gold_doc_decompx_tokens</th>\n",
       "      <th>gold_doc_decompx_tokenizer_word_ids</th>\n",
       "      <th>gold_doc_decompx_cls_or_mean_pooled</th>\n",
       "      <th>gold_doc_decompx_tokens_dot_scores</th>\n",
       "      <th>gold_doc_decompx_decompx_last_layer_pooled</th>\n",
       "      <th>pred_doc_decompx_tokens</th>\n",
       "      <th>pred_doc_decompx_tokenizer_word_ids</th>\n",
       "      <th>pred_doc_decompx_cls_or_mean_pooled</th>\n",
       "      <th>pred_doc_decompx_tokens_dot_scores</th>\n",
       "      <th>pred_doc_decompx_decompx_last_layer_pooled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test0</td>\n",
       "      <td>When was Loud Tour published?</td>\n",
       "      <td>[Loud Tour]</td>\n",
       "      <td>{'Loud Tour': {'text': 'The Loud Tour was the ...</td>\n",
       "      <td>{'len': 1000, 'max': 390.3378601074219, 'min':...</td>\n",
       "      <td>{'Loud Tour': 390.3378601074219}</td>\n",
       "      <td>{'Loud Tour': 390.3378601074219, 'Loud'n'proud...</td>\n",
       "      <td>{'Loud Tour': {'text': 'The Loud Tour was the ...</td>\n",
       "      <td>test0</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>[[{'name': 'Loud', 'pos': [23, 24], 'sent_id':...</td>\n",
       "      <td>[{'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]...</td>\n",
       "      <td>[[The, Loud, Tour, was, the, fourth, overall, ...</td>\n",
       "      <td>test</td>\n",
       "      <td>{'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]}</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'name': 'Loud', 'pos': [23, 24], 'sent_id': ...</td>\n",
       "      <td>[{'pos': [25, 26], 'type': 'TIME', 'sent_id': ...</td>\n",
       "      <td>{Loud Tour, Loud}</td>\n",
       "      <td>{2010}</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>2010</td>\n",
       "      <td>{MISC}</td>\n",
       "      <td>{TIME}</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[[Performing, in, over, twenty, countries, in,...</td>\n",
       "      <td>[{'name': 'Loud', 'pos': [23, 24], 'sent_id': ...</td>\n",
       "      <td>[{'pos': [25, 26], 'type': 'TIME', 'sent_id': ...</td>\n",
       "      <td>P577</td>\n",
       "      <td>publication date</td>\n",
       "      <td>When was Loud Tour published?</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Loud Tour The Loud Tour was the fourth overall...</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>The Loud Tour was the fourth overall and third...</td>\n",
       "      <td>390.337860</td>\n",
       "      <td>Loud Tour The Loud Tour was the fourth overall...</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>The Loud Tour was the fourth overall and third...</td>\n",
       "      <td>390.337860</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>[[CLS], when, was, loud, tour, published, ?, [...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 4, None]</td>\n",
       "      <td>[-0.17805682, -0.3927267, 0.34883702, -0.38739...</td>\n",
       "      <td>[2.2196622, 6.71451, 0.9866385, 58.316944, 37....</td>\n",
       "      <td>[[0.0026502553, 0.044497166, 0.009840142, -0.0...</td>\n",
       "      <td>[[CLS], loud, tour, the, loud, tour, was, the,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-0.7096514, -0.43747085, 2.078466, -0.8606712...</td>\n",
       "      <td>[650.5565, 112.46794, 110.70713, 35.217003, 88...</td>\n",
       "      <td>[[-0.06098142, 0.030208647, 0.35368052, -0.157...</td>\n",
       "      <td>[[CLS], loud, tour, the, loud, tour, was, the,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-0.7096514, -0.43747085, 2.078466, -0.8606712...</td>\n",
       "      <td>[650.5565, 112.46794, 110.70713, 35.217003, 88...</td>\n",
       "      <td>[[-0.06098142, 0.030208647, 0.35368052, -0.157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test1</td>\n",
       "      <td>Who performed Loud Tour?</td>\n",
       "      <td>[Loud Tour]</td>\n",
       "      <td>{'Loud Tour': {'text': 'The Loud Tour was the ...</td>\n",
       "      <td>{'len': 1000, 'max': 398.40228271484375, 'min'...</td>\n",
       "      <td>{'Loud Tour': 398.40228271484375}</td>\n",
       "      <td>{'Loud Tour': 398.40228271484375, 'Tonnage Tou...</td>\n",
       "      <td>{'Loud Tour': {'text': 'The Loud Tour was the ...</td>\n",
       "      <td>test1</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>[[{'name': 'Loud', 'pos': [23, 24], 'sent_id':...</td>\n",
       "      <td>[{'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]...</td>\n",
       "      <td>[[The, Loud, Tour, was, the, fourth, overall, ...</td>\n",
       "      <td>test</td>\n",
       "      <td>{'r': 'P175', 'h': 0, 't': 2, 'evidence': [0, 1]}</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'name': 'Loud', 'pos': [23, 24], 'sent_id': ...</td>\n",
       "      <td>[{'name': 'Rihanna', 'pos': [3, 4], 'sent_id':...</td>\n",
       "      <td>{Loud Tour, Loud}</td>\n",
       "      <td>{Rihanna}</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>Rihanna</td>\n",
       "      <td>{MISC}</td>\n",
       "      <td>{PER}</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[[The, Loud, Tour, was, the, fourth, overall, ...</td>\n",
       "      <td>[{'name': 'Loud', 'pos': [23, 24], 'sent_id': ...</td>\n",
       "      <td>[{'name': 'Rihanna', 'pos': [18, 19], 'sent_id...</td>\n",
       "      <td>P175</td>\n",
       "      <td>performer</td>\n",
       "      <td>Who performed Loud Tour?</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Loud Tour The Loud Tour was the fourth overall...</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>The Loud Tour was the fourth overall and third...</td>\n",
       "      <td>398.402283</td>\n",
       "      <td>Loud Tour The Loud Tour was the fourth overall...</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>The Loud Tour was the fourth overall and third...</td>\n",
       "      <td>398.402283</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>[[CLS], who, performed, loud, tour, ?, [SEP]]</td>\n",
       "      <td>[None, 0, 1, 2, 3, 3, None]</td>\n",
       "      <td>[-0.20367393, -0.43282467, 0.085645154, -0.127...</td>\n",
       "      <td>[1.0651449, 2.8293247, 14.008613, 61.79544, 30...</td>\n",
       "      <td>[[0.014361359, 0.08712164, 0.015172923, -0.007...</td>\n",
       "      <td>[[CLS], loud, tour, the, loud, tour, was, the,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-0.7096514, -0.43747085, 2.078466, -0.8606712...</td>\n",
       "      <td>[650.5565, 112.46794, 110.70713, 35.217003, 88...</td>\n",
       "      <td>[[-0.06098142, 0.030208647, 0.35368052, -0.157...</td>\n",
       "      <td>[[CLS], loud, tour, the, loud, tour, was, the,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-0.7096514, -0.43747085, 2.078466, -0.8606712...</td>\n",
       "      <td>[650.5565, 112.46794, 110.70713, 35.217003, 88...</td>\n",
       "      <td>[[-0.06098142, 0.030208647, 0.35368052, -0.157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test2</td>\n",
       "      <td>Which administrative territorial entity is The...</td>\n",
       "      <td>[Loud Tour]</td>\n",
       "      <td>{'Loud Tour': {'text': 'The Loud Tour was the ...</td>\n",
       "      <td>{'len': 1000, 'max': 379.07220458984375, 'min'...</td>\n",
       "      <td>{'Loud Tour': None}</td>\n",
       "      <td>{'Olympic Delivery Authority': 379.07220458984...</td>\n",
       "      <td>{'Olympic Delivery Authority': {'text': 'The O...</td>\n",
       "      <td>test2</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>[[{'name': 'Loud', 'pos': [23, 24], 'sent_id':...</td>\n",
       "      <td>[{'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]...</td>\n",
       "      <td>[[The, Loud, Tour, was, the, fourth, overall, ...</td>\n",
       "      <td>test</td>\n",
       "      <td>{'r': 'P131', 'h': 10, 't': 8, 'evidence': [4]}</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'sent_id': 4, 'type': 'LOC', 'pos': [11, 14]...</td>\n",
       "      <td>[{'name': 'London', 'pos': [1, 2], 'sent_id': ...</td>\n",
       "      <td>{The O2 Arena}</td>\n",
       "      <td>{London}</td>\n",
       "      <td>The O2 Arena</td>\n",
       "      <td>London</td>\n",
       "      <td>{LOC}</td>\n",
       "      <td>{LOC}</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[[In, London, ,, Rihanna, played, a, record, b...</td>\n",
       "      <td>[{'sent_id': 4, 'type': 'LOC', 'pos': [11, 14]...</td>\n",
       "      <td>[{'name': 'London', 'pos': [1, 2], 'sent_id': ...</td>\n",
       "      <td>P131</td>\n",
       "      <td>located in the administrative territorial entity</td>\n",
       "      <td>Which administrative territorial entity is The...</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>inf</td>\n",
       "      <td>Loud Tour The Loud Tour was the fourth overall...</td>\n",
       "      <td>Loud Tour</td>\n",
       "      <td>The Loud Tour was the fourth overall and third...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Olympic Delivery Authority The Olympic Deliver...</td>\n",
       "      <td>Olympic Delivery Authority</td>\n",
       "      <td>The Olympic Delivery Authority ( ODA ) was a n...</td>\n",
       "      <td>379.072205</td>\n",
       "      <td>142</td>\n",
       "      <td>226</td>\n",
       "      <td>[[CLS], which, administrative, territorial, en...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 9, None]</td>\n",
       "      <td>[-0.033545855, 0.04599817, -0.09456632, -0.027...</td>\n",
       "      <td>[6.398443, -1.6474726, 4.8608465, 11.063667, 1...</td>\n",
       "      <td>[[0.0017761212, 0.025855744, 0.023759678, -0.0...</td>\n",
       "      <td>[[CLS], loud, tour, the, loud, tour, was, the,...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
       "      <td>[-0.7096514, -0.43747085, 2.078466, -0.8606712...</td>\n",
       "      <td>[650.5565, 112.46794, 110.70713, 35.217003, 88...</td>\n",
       "      <td>[[-0.06098142, 0.030208647, 0.35368052, -0.157...</td>\n",
       "      <td>[[CLS], olympic, delivery, authority, the, oly...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11...</td>\n",
       "      <td>[-0.56008214, 0.15913305, 1.5809196, -0.728217...</td>\n",
       "      <td>[771.9823, 40.083252, 107.10669, 9.972132, 17....</td>\n",
       "      <td>[[-0.06645119, 0.03153456, 0.4184458, -0.19593...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                              query    gold_docs  \\\n",
       "0    test0                      When was Loud Tour published?  [Loud Tour]   \n",
       "1    test1                           Who performed Loud Tour?  [Loud Tour]   \n",
       "2    test2  Which administrative territorial entity is The...  [Loud Tour]   \n",
       "\n",
       "                                      gold_docs_text  \\\n",
       "0  {'Loud Tour': {'text': 'The Loud Tour was the ...   \n",
       "1  {'Loud Tour': {'text': 'The Loud Tour was the ...   \n",
       "2  {'Loud Tour': {'text': 'The Loud Tour was the ...   \n",
       "\n",
       "                                        scores_stats  \\\n",
       "0  {'len': 1000, 'max': 390.3378601074219, 'min':...   \n",
       "1  {'len': 1000, 'max': 398.40228271484375, 'min'...   \n",
       "2  {'len': 1000, 'max': 379.07220458984375, 'min'...   \n",
       "\n",
       "                         scores_gold  \\\n",
       "0   {'Loud Tour': 390.3378601074219}   \n",
       "1  {'Loud Tour': 398.40228271484375}   \n",
       "2                {'Loud Tour': None}   \n",
       "\n",
       "                                         scores_1000  \\\n",
       "0  {'Loud Tour': 390.3378601074219, 'Loud'n'proud...   \n",
       "1  {'Loud Tour': 398.40228271484375, 'Tonnage Tou...   \n",
       "2  {'Olympic Delivery Authority': 379.07220458984...   \n",
       "\n",
       "                              predicted_docs_text_10     id      title  \\\n",
       "0  {'Loud Tour': {'text': 'The Loud Tour was the ...  test0  Loud Tour   \n",
       "1  {'Loud Tour': {'text': 'The Loud Tour was the ...  test1  Loud Tour   \n",
       "2  {'Olympic Delivery Authority': {'text': 'The O...  test2  Loud Tour   \n",
       "\n",
       "                                           vertexSet  \\\n",
       "0  [[{'name': 'Loud', 'pos': [23, 24], 'sent_id':...   \n",
       "1  [[{'name': 'Loud', 'pos': [23, 24], 'sent_id':...   \n",
       "2  [[{'name': 'Loud', 'pos': [23, 24], 'sent_id':...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [{'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]...   \n",
       "1  [{'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]...   \n",
       "2  [{'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]...   \n",
       "\n",
       "                                               sents split  \\\n",
       "0  [[The, Loud, Tour, was, the, fourth, overall, ...  test   \n",
       "1  [[The, Loud, Tour, was, the, fourth, overall, ...  test   \n",
       "2  [[The, Loud, Tour, was, the, fourth, overall, ...  test   \n",
       "\n",
       "                                               label  label_idx  \\\n",
       "0     {'r': 'P577', 'h': 0, 't': 6, 'evidence': [1]}          0   \n",
       "1  {'r': 'P175', 'h': 0, 't': 2, 'evidence': [0, 1]}          1   \n",
       "2    {'r': 'P131', 'h': 10, 't': 8, 'evidence': [4]}          2   \n",
       "\n",
       "                                         head_entity  \\\n",
       "0  [{'name': 'Loud', 'pos': [23, 24], 'sent_id': ...   \n",
       "1  [{'name': 'Loud', 'pos': [23, 24], 'sent_id': ...   \n",
       "2  [{'sent_id': 4, 'type': 'LOC', 'pos': [11, 14]...   \n",
       "\n",
       "                                         tail_entity  head_entity_names  \\\n",
       "0  [{'pos': [25, 26], 'type': 'TIME', 'sent_id': ...  {Loud Tour, Loud}   \n",
       "1  [{'name': 'Rihanna', 'pos': [3, 4], 'sent_id':...  {Loud Tour, Loud}   \n",
       "2  [{'name': 'London', 'pos': [1, 2], 'sent_id': ...     {The O2 Arena}   \n",
       "\n",
       "  tail_entity_names head_entity_longest_name tail_entity_longest_name  \\\n",
       "0            {2010}                Loud Tour                     2010   \n",
       "1         {Rihanna}                Loud Tour                  Rihanna   \n",
       "2          {London}             The O2 Arena                   London   \n",
       "\n",
       "  head_entity_types tail_entity_types evidence_sent_ids  \\\n",
       "0            {MISC}            {TIME}               [1]   \n",
       "1            {MISC}             {PER}            [0, 1]   \n",
       "2             {LOC}             {LOC}               [4]   \n",
       "\n",
       "                                      evidence_sents  \\\n",
       "0  [[Performing, in, over, twenty, countries, in,...   \n",
       "1  [[The, Loud, Tour, was, the, fourth, overall, ...   \n",
       "2  [[In, London, ,, Rihanna, played, a, record, b...   \n",
       "\n",
       "                             head_entity_in_evidence  \\\n",
       "0  [{'name': 'Loud', 'pos': [23, 24], 'sent_id': ...   \n",
       "1  [{'name': 'Loud', 'pos': [23, 24], 'sent_id': ...   \n",
       "2  [{'sent_id': 4, 'type': 'LOC', 'pos': [11, 14]...   \n",
       "\n",
       "                             tail_entity_in_evidence relation  \\\n",
       "0  [{'pos': [25, 26], 'type': 'TIME', 'sent_id': ...     P577   \n",
       "1  [{'name': 'Rihanna', 'pos': [18, 19], 'sent_id...     P175   \n",
       "2  [{'name': 'London', 'pos': [1, 2], 'sent_id': ...     P131   \n",
       "\n",
       "                                      relation_name  \\\n",
       "0                                  publication date   \n",
       "1                                         performer   \n",
       "2  located in the administrative territorial entity   \n",
       "\n",
       "                                      query_question  duplicate_titles_len  \\\n",
       "0                      When was Loud Tour published?                     0   \n",
       "1                           Who performed Loud Tour?                     0   \n",
       "2  Which administrative territorial entity is The...                     0   \n",
       "\n",
       "  duplicate_titles  hit_rank  \\\n",
       "0               {}       1.0   \n",
       "1               {}       1.0   \n",
       "2               {}       inf   \n",
       "\n",
       "                                            gold_doc gold_doc_title  \\\n",
       "0  Loud Tour The Loud Tour was the fourth overall...      Loud Tour   \n",
       "1  Loud Tour The Loud Tour was the fourth overall...      Loud Tour   \n",
       "2  Loud Tour The Loud Tour was the fourth overall...      Loud Tour   \n",
       "\n",
       "                                       gold_doc_text  gold_doc_score  \\\n",
       "0  The Loud Tour was the fourth overall and third...      390.337860   \n",
       "1  The Loud Tour was the fourth overall and third...      398.402283   \n",
       "2  The Loud Tour was the fourth overall and third...             NaN   \n",
       "\n",
       "                                            pred_doc  \\\n",
       "0  Loud Tour The Loud Tour was the fourth overall...   \n",
       "1  Loud Tour The Loud Tour was the fourth overall...   \n",
       "2  Olympic Delivery Authority The Olympic Deliver...   \n",
       "\n",
       "               pred_doc_title  \\\n",
       "0                   Loud Tour   \n",
       "1                   Loud Tour   \n",
       "2  Olympic Delivery Authority   \n",
       "\n",
       "                                       pred_doc_text  pred_doc_score  \\\n",
       "0  The Loud Tour was the fourth overall and third...      390.337860   \n",
       "1  The Loud Tour was the fourth overall and third...      398.402283   \n",
       "2  The Olympic Delivery Authority ( ODA ) was a n...      379.072205   \n",
       "\n",
       "   gold_doc_len  pred_doc_len  \\\n",
       "0           142           142   \n",
       "1           142           142   \n",
       "2           142           226   \n",
       "\n",
       "                                query_decompx_tokens  \\\n",
       "0  [[CLS], when, was, loud, tour, published, ?, [...   \n",
       "1      [[CLS], who, performed, loud, tour, ?, [SEP]]   \n",
       "2  [[CLS], which, administrative, territorial, en...   \n",
       "\n",
       "                   query_decompx_tokenizer_word_ids  \\\n",
       "0                    [None, 0, 1, 2, 3, 4, 4, None]   \n",
       "1                       [None, 0, 1, 2, 3, 3, None]   \n",
       "2  [None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 9, None]   \n",
       "\n",
       "                    query_decompx_cls_or_mean_pooled  \\\n",
       "0  [-0.17805682, -0.3927267, 0.34883702, -0.38739...   \n",
       "1  [-0.20367393, -0.43282467, 0.085645154, -0.127...   \n",
       "2  [-0.033545855, 0.04599817, -0.09456632, -0.027...   \n",
       "\n",
       "                     query_decompx_tokens_dot_scores  \\\n",
       "0  [2.2196622, 6.71451, 0.9866385, 58.316944, 37....   \n",
       "1  [1.0651449, 2.8293247, 14.008613, 61.79544, 30...   \n",
       "2  [6.398443, -1.6474726, 4.8608465, 11.063667, 1...   \n",
       "\n",
       "             query_decompx_decompx_last_layer_pooled  \\\n",
       "0  [[0.0026502553, 0.044497166, 0.009840142, -0.0...   \n",
       "1  [[0.014361359, 0.08712164, 0.015172923, -0.007...   \n",
       "2  [[0.0017761212, 0.025855744, 0.023759678, -0.0...   \n",
       "\n",
       "                             gold_doc_decompx_tokens  \\\n",
       "0  [[CLS], loud, tour, the, loud, tour, was, the,...   \n",
       "1  [[CLS], loud, tour, the, loud, tour, was, the,...   \n",
       "2  [[CLS], loud, tour, the, loud, tour, was, the,...   \n",
       "\n",
       "                 gold_doc_decompx_tokenizer_word_ids  \\\n",
       "0  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "1  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "2  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "\n",
       "                 gold_doc_decompx_cls_or_mean_pooled  \\\n",
       "0  [-0.7096514, -0.43747085, 2.078466, -0.8606712...   \n",
       "1  [-0.7096514, -0.43747085, 2.078466, -0.8606712...   \n",
       "2  [-0.7096514, -0.43747085, 2.078466, -0.8606712...   \n",
       "\n",
       "                  gold_doc_decompx_tokens_dot_scores  \\\n",
       "0  [650.5565, 112.46794, 110.70713, 35.217003, 88...   \n",
       "1  [650.5565, 112.46794, 110.70713, 35.217003, 88...   \n",
       "2  [650.5565, 112.46794, 110.70713, 35.217003, 88...   \n",
       "\n",
       "          gold_doc_decompx_decompx_last_layer_pooled  \\\n",
       "0  [[-0.06098142, 0.030208647, 0.35368052, -0.157...   \n",
       "1  [[-0.06098142, 0.030208647, 0.35368052, -0.157...   \n",
       "2  [[-0.06098142, 0.030208647, 0.35368052, -0.157...   \n",
       "\n",
       "                             pred_doc_decompx_tokens  \\\n",
       "0  [[CLS], loud, tour, the, loud, tour, was, the,...   \n",
       "1  [[CLS], loud, tour, the, loud, tour, was, the,...   \n",
       "2  [[CLS], olympic, delivery, authority, the, oly...   \n",
       "\n",
       "                 pred_doc_decompx_tokenizer_word_ids  \\\n",
       "0  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "1  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...   \n",
       "2  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11...   \n",
       "\n",
       "                 pred_doc_decompx_cls_or_mean_pooled  \\\n",
       "0  [-0.7096514, -0.43747085, 2.078466, -0.8606712...   \n",
       "1  [-0.7096514, -0.43747085, 2.078466, -0.8606712...   \n",
       "2  [-0.56008214, 0.15913305, 1.5809196, -0.728217...   \n",
       "\n",
       "                  pred_doc_decompx_tokens_dot_scores  \\\n",
       "0  [650.5565, 112.46794, 110.70713, 35.217003, 88...   \n",
       "1  [650.5565, 112.46794, 110.70713, 35.217003, 88...   \n",
       "2  [771.9823, 40.083252, 107.10669, 9.972132, 17....   \n",
       "\n",
       "          pred_doc_decompx_decompx_last_layer_pooled  \n",
       "0  [[-0.06098142, 0.030208647, 0.35368052, -0.157...  \n",
       "1  [[-0.06098142, 0.030208647, 0.35368052, -0.157...  \n",
       "2  [[-0.06645119, 0.03153456, 0.4184458, -0.19593...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_pickle(f\"./hf/results/{DATASET}\")\n",
    "print(df_raw.attrs)\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2239\n",
      "Who performed Long Hard Road Out of Hell?\n",
      "[{'name': 'Sneaker Pimps', 'pos': [22, 24], 'sent_id': 0, 'type': 'ORG', 'global_pos': [22, 22], 'index': '4_0'}]\n",
      "[{'name': 'Long Hard Road Out of Hell', 'pos': [1, 7], 'sent_id': 0, 'type': 'MISC', 'global_pos': [1, 1], 'index': '0_0'}]\n",
      "0 ['\"', 'Long', 'Hard', 'Road', 'Out', 'of', 'Hell', '\"', 'is', 'a', 'song', 'by', 'American', 'rock', 'band', 'Marilyn', 'Manson', 'and', 'British', 'trip', 'hop', 'band', 'Sneaker', 'Pimps', ',', 'released', 'as', 'a', 'single', 'from', 'the', 'soundtrack', 'to', 'the', '1997', 'motion', 'picture', 'Spawn', '.']\n",
      "1 ['An', 'arena', 'rock', 'and', 'gothic', 'rock', 'song', ',', '\"', 'Long', 'Hard', 'Road', 'Out', 'of', 'Hell', '\"', 'was', 'written', 'by', 'Marilyn', 'Manson', 'and', 'Twiggy', 'Ramirez', 'and', 'produced', 'by', 'Manson', 'and', 'Sean', 'Beavan', '.']\n",
      "2 ['Its', 'lyrics', 'are', 'about', 'self', '-', 'loathing', 'and', 'its', 'title', 'is', 'derived', 'from', 'John', 'Milton', \"'s\", 'Paradise', 'Lost', '(', '1667', ')', '.']\n",
      "3 ['After', 'the', 'track', 'was', 'written', ',', 'the', 'Sneaker', 'Pimps', \"'\", 'Kelli', 'Ali', 'was', 'recruited', 'to', 'perform', 'background', 'vocals', 'on', 'it', ',', 'as', 'the', 'Spawn', 'soundtrack', 'featured', 'collaborations', 'between', 'hard', 'rock', 'artists', 'and', 'electronic', 'music', 'artists', '.']\n",
      "4 ['The', 'Sneaker', 'Pimps', 'were', 'dissatisfied', 'with', 'the', 'final', 'track', 'and', 'wanted', 'a', 'remix', 'of', 'it', 'to', 'be', 'released', 'as', 'a', 'single', 'instead', ';', 'conversely', ',', 'Manson', 'deemed', 'it', 'a', 'personal', 'favorite', '.']\n",
      "5 ['\"', 'Long', 'Hard', 'Road', 'Out', 'of', 'Hell', '\"', 'received', 'mixed', 'reviews', 'from', 'music', 'critics', ';', 'some', 'found', 'it', 'heartfelt', 'while', 'others', 'felt', 'it', 'was', 'too', 'indistinct', 'from', 'other', 'Marilyn', 'Manson', 'songs', '.']\n",
      "6 ['Commentators', 'noted', 'that', 'the', 'track', 'encapsulated', 'the', 'evolution', 'of', 'the', 'band', \"'s\", 'sound', 'from', 'the', 'industrial', 'music', 'of', 'Antichrist', 'Superstar', '(', '1996', ')', 'to', 'the', 'glam', 'rock', 'of', 'Mechanical', 'Animals', '(', '1998', ')', '.']\n",
      "7 ['Manson', 'initially', 'approached', 'Jonathan', 'Glazer', 'to', 'direct', 'the', 'video', 'for', 'the', 'track', ',', 'but', 'rejected', 'his', 'concept', 'for', 'it', ';', 'Glazer', 'later', 'used', 'his', 'concept', 'for', 'the', 'video', 'for', 'Radiohead', \"'s\", '\"', 'Karma', 'Police', '\"', '(', '1997', ')', '.']\n",
      "8 ['The', 'music', 'video', 'for', '\"', 'Long', 'Hard', 'Road', 'Out', 'of', 'Hell', '\"', 'was', 'directed', 'by', 'Matthew', 'Rolston', '.']\n",
      "9 ['It', 'depicts', 'Manson', 'in', 'a', 'dress', 'and', 'a', 'group', 'of', 'models', 'who', 'initially', 'appear', 'to', 'be', 'female', 'but', 'are', 'revealed', 'to', 'be', 'male', '.']\n",
      "10 ['The', 'clip', 'garnerd', 'acclaim', 'from', 'critics', 'for', 'its', 'imagery', '.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>head_w_tail_sentence</th>\n",
       "      <th>head_wo_tail_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Who performed Long Hard Road Out of Hell?</td>\n",
       "      <td>\" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .</td>\n",
       "      <td>An arena rock and gothic rock song , \" Long Hard Road Out of Hell \" was written by Marilyn Manson and Twiggy Ramirez and produced by Manson and Sean Beavan .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>What is Long Hard Road Out of Hell a part of?</td>\n",
       "      <td>\" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .</td>\n",
       "      <td>An arena rock and gothic rock song , \" Long Hard Road Out of Hell \" was written by Marilyn Manson and Twiggy Ramirez and produced by Manson and Sean Beavan .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>When was Spawn published?</td>\n",
       "      <td>\" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .</td>\n",
       "      <td>After the track was written , the Sneaker Pimps ' Kelli Ali was recruited to perform background vocals on it , as the Spawn soundtrack featured collaborations between hard rock artists and electronic music artists .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>When was Long Hard Road Out of Hell published?</td>\n",
       "      <td>\" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .</td>\n",
       "      <td>An arena rock and gothic rock song , \" Long Hard Road Out of Hell \" was written by Marilyn Manson and Twiggy Ramirez and produced by Manson and Sean Beavan .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>What is a notable work of Sneaker Pimps?</td>\n",
       "      <td>\" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .</td>\n",
       "      <td>After the track was written , the Sneaker Pimps ' Kelli Ali was recruited to perform background vocals on it , as the Spawn soundtrack featured collaborations between hard rock artists and electronic music artists .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7032</th>\n",
       "      <td>Where was Kerstin Thorborg born?</td>\n",
       "      <td>Born in Venjan , Sweden , the contralto Kerstin Thorborg was one of the best dramatic Wagnerian singers in the two decades between 1930 and 1950 .</td>\n",
       "      <td>Kerstin Thorborg ( May 19 , 1896 - April 12 , 1970 )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>Which country is Cimatti associated with?</td>\n",
       "      <td>Cimatti was an Italian manufacturer of bicycles , motorcycles and mopeds active between 1937 and 1984 .</td>\n",
       "      <td>Cimatti used two - stroke engines bought from both Moto Morini and Moto Minarelli .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7090</th>\n",
       "      <td>Which administrative territorial entity is Cimatti located in?</td>\n",
       "      <td>Cimatti was an Italian manufacturer of bicycles , motorcycles and mopeds active between 1937 and 1984 .</td>\n",
       "      <td>Cimatti used two - stroke engines bought from both Moto Morini and Moto Minarelli .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7106</th>\n",
       "      <td>What is Line M1 a part of?</td>\n",
       "      <td>Młociny is a Warsaw Metro station serving as a northern terminus to Line M1 .</td>\n",
       "      <td>Although there are no plans to extend Line M1 further , the station is built in such way that it will be possible to do so if need be .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7111</th>\n",
       "      <td>What are the components of Line M1?</td>\n",
       "      <td>Młociny is a Warsaw Metro station serving as a northern terminus to Line M1 .</td>\n",
       "      <td>Although there are no plans to extend Line M1 further , the station is built in such way that it will be possible to do so if need be .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               query  \\\n",
       "32                         Who performed Long Hard Road Out of Hell?   \n",
       "33                     What is Long Hard Road Out of Hell a part of?   \n",
       "36                                         When was Spawn published?   \n",
       "37                    When was Long Hard Road Out of Hell published?   \n",
       "38                          What is a notable work of Sneaker Pimps?   \n",
       "...                                                              ...   \n",
       "7032                                Where was Kerstin Thorborg born?   \n",
       "7089                       Which country is Cimatti associated with?   \n",
       "7090  Which administrative territorial entity is Cimatti located in?   \n",
       "7106                                      What is Line M1 a part of?   \n",
       "7111                             What are the components of Line M1?   \n",
       "\n",
       "                                                                                                                                                                                     head_w_tail_sentence  \\\n",
       "32    \" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .   \n",
       "33    \" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .   \n",
       "36    \" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .   \n",
       "37    \" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .   \n",
       "38    \" Long Hard Road Out of Hell \" is a song by American rock band Marilyn Manson and British trip hop band Sneaker Pimps , released as a single from the soundtrack to the 1997 motion picture Spawn .   \n",
       "...                                                                                                                                                                                                   ...   \n",
       "7032                                                   Born in Venjan , Sweden , the contralto Kerstin Thorborg was one of the best dramatic Wagnerian singers in the two decades between 1930 and 1950 .   \n",
       "7089                                                                                              Cimatti was an Italian manufacturer of bicycles , motorcycles and mopeds active between 1937 and 1984 .   \n",
       "7090                                                                                              Cimatti was an Italian manufacturer of bicycles , motorcycles and mopeds active between 1937 and 1984 .   \n",
       "7106                                                                                                                        Młociny is a Warsaw Metro station serving as a northern terminus to Line M1 .   \n",
       "7111                                                                                                                        Młociny is a Warsaw Metro station serving as a northern terminus to Line M1 .   \n",
       "\n",
       "                                                                                                                                                                                                        head_wo_tail_sentence  \n",
       "32                                                              An arena rock and gothic rock song , \" Long Hard Road Out of Hell \" was written by Marilyn Manson and Twiggy Ramirez and produced by Manson and Sean Beavan .  \n",
       "33                                                              An arena rock and gothic rock song , \" Long Hard Road Out of Hell \" was written by Marilyn Manson and Twiggy Ramirez and produced by Manson and Sean Beavan .  \n",
       "36    After the track was written , the Sneaker Pimps ' Kelli Ali was recruited to perform background vocals on it , as the Spawn soundtrack featured collaborations between hard rock artists and electronic music artists .  \n",
       "37                                                              An arena rock and gothic rock song , \" Long Hard Road Out of Hell \" was written by Marilyn Manson and Twiggy Ramirez and produced by Manson and Sean Beavan .  \n",
       "38    After the track was written , the Sneaker Pimps ' Kelli Ali was recruited to perform background vocals on it , as the Spawn soundtrack featured collaborations between hard rock artists and electronic music artists .  \n",
       "...                                                                                                                                                                                                                       ...  \n",
       "7032                                                                                                                                                                     Kerstin Thorborg ( May 19 , 1896 - April 12 , 1970 )  \n",
       "7089                                                                                                                                      Cimatti used two - stroke engines bought from both Moto Morini and Moto Minarelli .  \n",
       "7090                                                                                                                                      Cimatti used two - stroke engines bought from both Moto Morini and Moto Minarelli .  \n",
       "7106                                                                                  Although there are no plans to extend Line M1 further , the station is built in such way that it will be possible to do so if need be .  \n",
       "7111                                                                                  Although there are no plans to extend Line M1 further , the station is built in such way that it will be possible to do so if need be .  \n",
       "\n",
       "[420 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "df = df[df[\"evidence_sent_ids\"].str.len() == 1]  # 1 Evidence\n",
    "df = df[df[\"head_entity_in_evidence\"].str.len() == 1]  # 1 Head in Evidence\n",
    "df = df[df[\"head_entity_names\"].str.len() == 1]  # All heads have the same name\n",
    "df = df[df[\"evidence_sents\"].str.len() == 1]  # 1 Evidence Sentence\n",
    "print(len(df))  # 2239\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "head_w_tail_sents = []\n",
    "head_wo_tail_sents = []\n",
    "for row in df.to_dict(orient=\"records\"):\n",
    "    head_w_tail = \" \".join(flatten(row[\"evidence_sents\"]))\n",
    "    head_wo_tail = None\n",
    "    for head_entity in row[\"head_entity\"]:\n",
    "        evidence_sent_id = row[\"evidence_sent_ids\"][0]\n",
    "        if head_entity[\"sent_id\"] != evidence_sent_id:\n",
    "            head_wo_tail = \" \".join(row[\"sents\"][head_entity[\"sent_id\"]])\n",
    "            break\n",
    "    if head_wo_tail is None:\n",
    "        head_w_tail_sents.append(None)\n",
    "        head_wo_tail_sents.append(None)\n",
    "    else:\n",
    "        head_w_tail_sents.append(head_w_tail)\n",
    "        head_wo_tail_sents.append(head_wo_tail)\n",
    "    \n",
    "df[\"head_w_tail_sentence\"] = head_w_tail_sents\n",
    "df[\"head_wo_tail_sentence\"] = head_wo_tail_sents\n",
    "df = df.dropna(subset=[\"head_w_tail_sentence\", \"head_wo_tail_sentence\"])\n",
    "\n",
    "d = df.iloc[0].to_dict()\n",
    "print(d[\"query\"])\n",
    "print(d[\"tail_entity_in_evidence\"])\n",
    "print(d[\"head_entity_in_evidence\"])\n",
    "for i, d in enumerate(d[\"sents\"]): print(i, d)\n",
    "\n",
    "df[[\"query\", \"head_w_tail_sentence\", \"head_wo_tail_sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourCustomDEModel:\n",
    "    def __init__(self, q_model, doc_model, pooling, sep: str = \" \", **kwargs):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(q_model)\n",
    "        self.query_encoder = AutoModel.from_pretrained(q_model)\n",
    "        self.context_encoder = AutoModel.from_pretrained(doc_model)\n",
    "        self.pooling = pooling\n",
    "        self.sep = sep\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Write your own encoding query function (Returns: Query embeddings as numpy array)\n",
    "    def encode_queries(self, queries: List[str], batch_size=128, **kwargs) -> np.ndarray:\n",
    "        print(\"Q\")\n",
    "        print(len(queries))\n",
    "        return self.encode_in_batch(self.query_encoder, queries, batch_size)\n",
    "    \n",
    "    # Write your own encoding corpus function (Returns: Document embeddings as numpy array)  \n",
    "    def encode_corpus(self, corpus: List[Dict[str, str]], batch_size=128, **kwargs) -> np.ndarray:\n",
    "        if type(corpus) is dict:\n",
    "            sentences = [(corpus[\"title\"][i] + self.sep + corpus[\"text\"][i]).strip() if \"title\" in corpus else corpus[\"text\"][i].strip() for i in range(len(corpus['text']))]\n",
    "        else:\n",
    "            sentences = [(doc[\"title\"] + self.sep + doc[\"text\"]).strip() if \"title\" in doc else doc[\"text\"].strip() for doc in corpus]\n",
    "        return self.encode_in_batch(self.context_encoder, sentences, batch_size)\n",
    "\n",
    "    def encode_in_batch(self, model, sentences: List[str], batch_size=128, **kwargs) -> np.ndarray:\n",
    "        model.to(self.device)\n",
    "        all_embeddings = []\n",
    "        for batch in tqdm(torch.utils.data.DataLoader(sentences, batch_size=batch_size, shuffle=False)):\n",
    "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            ### POOLING\n",
    "            if self.pooling == \"avg\":\n",
    "                embeddings = self.mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "            elif self.pooling == \"cls\":\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]  # [128, 768] = [batch, emb_dim]\n",
    "            else:\n",
    "                raise ValueError(\"Pooling method not supported\")\n",
    "            all_embeddings.extend(embeddings.detach().cpu().numpy())\n",
    "        all_embeddings = np.array(all_embeddings)\n",
    "        print(all_embeddings.shape)\n",
    "        return all_embeddings\n",
    "\n",
    "    def mean_pooling(self, token_embeddings, mask):\n",
    "        token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "        sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DRAGON\n",
    "query_model = \"facebook/dragon-plus-query-encoder\"\n",
    "context_model = \"facebook/dragon-plus-context-encoder\"\n",
    "POOLING = \"cls\"\n",
    "\n",
    "dpr = YourCustomDEModel(query_model, context_model, POOLING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      "420\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bd60b1a3fa4833983cd0462cb1298e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 768)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015bf153d9534473ab147449e970e59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 14.81 MiB is free. Process 3277830 has 40.25 GiB memory in use. Including non-PyTorch memory, this process has 7.04 GiB memory in use. Of the allocated memory 6.52 GiB is allocated by PyTorch, and 226.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: s} \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[1;32m      4\u001b[0m query_embds \u001b[38;5;241m=\u001b[39m dpr\u001b[38;5;241m.\u001b[39mencode_queries(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list())\n\u001b[0;32m----> 5\u001b[0m head_w_tail_embds \u001b[38;5;241m=\u001b[39m \u001b[43mdpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_doc_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead_w_tail_sentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m head_wo_tail_embds \u001b[38;5;241m=\u001b[39m dpr\u001b[38;5;241m.\u001b[39mencode_corpus(to_doc_format(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_wo_tail_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()))\n",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m, in \u001b[0;36mYourCustomDEModel.encode_corpus\u001b[0;34m(self, corpus, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [(doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01melse\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_in_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 30\u001b[0m, in \u001b[0;36mYourCustomDEModel.encode_in_batch\u001b[0;34m(self, model, sentences, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(batch, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     29\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: val\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m### POOLING\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data2/mohsenfayyaz/anaconda3/envs/mohsen-env/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:554\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    552\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    553\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 14.81 MiB is free. Process 3277830 has 40.25 GiB memory in use. Including non-PyTorch memory, this process has 7.04 GiB memory in use. Of the allocated memory 6.52 GiB is allocated by PyTorch, and 226.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def to_doc_format(sentences: list):\n",
    "    return [{\"text\": s} for s in sentences]\n",
    "\n",
    "query_embds = dpr.encode_queries(df[\"query\"].to_list())\n",
    "head_w_tail_embds = dpr.encode_corpus(to_doc_format(df[\"head_w_tail_sentence\"].to_list()))\n",
    "head_wo_tail_embds = dpr.encode_corpus(to_doc_format(df[\"head_wo_tail_sentence\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mohsen-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
